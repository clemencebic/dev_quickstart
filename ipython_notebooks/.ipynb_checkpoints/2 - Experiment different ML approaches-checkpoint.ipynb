{
  "metadata": {
    "analyzedDataset": "heart_measures_train",
    "createdOn": 1682585335302,
    "creator": "clemence.bic@dataiku.com",
    "customFields": {},
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "admin",
    "tags": [],
    "versionNumber": 1
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Test different Machine Learning models for heart failures prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will test different Machine Learning approaches to predict heart failures using [scikit-learn](https://scikit-learn.org/stable/)  models (logistic regression, SVM, decision tree, and random forest). For each model, we will first perform a grid search to find the best parameters, then train the model on the train set using these best parameters and finally log everything (parameters, performance metrics, and models) to keep track of the results of our different experiments and be able to compare afterward.\nOur [Experiment Tracking capability](https://doc.dataiku.com/dss/latest/mlops/experiment-tracking/index.html) relies on the [MLFlow framework](https://www.mlflow.org/docs/1.30.0/tracking.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n\u003cb\u003eTip:\u003c/b\u003e Experiment Tracking allows you to save all experiment-related information that you care about for every experiment you run. In Dataiku, this can be done when coding using the \u003ca href\u003d\"https://www.mlflow.org/docs/1.30.0/tracking.html\"\u003eMLFlow Tracking API\u003c/a\u003e. You can then explore and compare all your experiments in the \u003ca href\u003d\"https://doc.dataiku.com/dss/latest/mlops/experiment-tracking/viewing.html\"\u003eExperiment Tracking UI\u003c/a\u003e.\n\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Make sure you\u0027re using the `heart-attack-project` code environment** (see prerequisites)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd\nfrom heart_attack_library import model_training\nimport mlflow\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\nwarnings.filterwarnings(\u0027ignore\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import the train dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_heart_measures_train \u003d dataiku.Dataset(\"heart_measures_train\")\ndf \u003d dataset_heart_measures_train.get_dataframe(limit\u003d100000)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set the experiment environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we would like to keep track of all the experiment-related information (performance metrics, parameters and models) for our different ML experiments, we must use a Dataiku managed folder to store all this information. This section is about creating (or accessing if it already exists) the required managed folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Set the required parameters for creating/accessing the managed folder"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set parameters\nexperiment_name \u003d \"Binary Heart Disease Classification\"\nexperiments_managed_folder_name \u003d \"Binary classif experiments\"\nproject \u003d dataiku.api_client().get_default_project()\nmlflow_extension \u003d project.get_mlflow_extension()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Create/access the managed folder"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the managed folder if it doesn\u0027t exist\nif experiments_managed_folder_name not in [folder[\u0027name\u0027] for folder in project.list_managed_folders()]:\n    project.create_managed_folder(experiments_managed_folder_name)\n    \n# Get the managed folder id\nexperiments_managed_folder_id \u003d [folder[\u0027id\u0027] for folder in project.list_managed_folders() if folder[\u0027name\u0027]\u003d\u003dexperiments_managed_folder_name][0] \n\n# Get the managed folder using the id\nexperiments_managed_folder \u003d project.get_managed_folder(experiments_managed_folder_id)  "
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Prepare data for training"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare data for experiment\ntarget\u003d [\"HeartDisease\"]\nX \u003d df.drop(target, axis\u003d1)\ny \u003d df[target[0]]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test different modeling approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section will test different models: a Logistic Regression, an SVM, a Decision Tree, and a Random Forest. For each type of model, we will proceed in several steps:\n\n1. Set the experiment (where to log the results) and start a new run.\n\n2. Define the set of hyperparameters to test.\n\n3. Perform a grid search on these hyperparameters using the ``find_best_parameters`` function from the ``model_training.py`` file in the project library.\n\t\n\n4. Cross-evaluate the model with the best hyperparameters on 5 folds using the ```cross_validate_scores``` function from the ```model_training.py``` file in the project library.\n\n5. Train the model on the train set using the best hyperparameters.\n\n6. Log the experiment\u0027s results (parameters, performance metrics, and model).\n\n\nYou can find more information on the tracking APIs in the [MLFlow tracking documentation](https://www.mlflow.org/docs/1.30.0/tracking.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n    mlflow.set_experiment(experiment_name)\n\n    with mlflow.start_run(run_name\u003d\"Linear Regression\"):\n        \n        # Find best hyper parameters using a grid search\n        lr \u003d LogisticRegression(random_state \u003d 42)\n        cv \u003d 5\n        params \u003d {\u0027penalty\u0027:[\u0027none\u0027,\u0027l2\u0027]}\n        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n        print(\"Searching for best parameters...\")\n        lr_best_params \u003d model_training.find_best_parameters(X, y, lr, params, cv\u003dcv)\n        print(f\"Best parameters: {lr_best_params}\")\n        \n        # Set the best hyper parameters\n        lr.set_params(**lr_best_params)\n        \n        # Cross evaluate the model on the best hyper parameters\n        lr_metrics_results \u003d model_training.cross_validate_scores(X, y, lr, cv\u003dcv, scoring\u003dscoring)\n        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in lr_metrics_results.items())}\u0027)\n        \n        # Train the model on the whole train set\n        lr.fit(X,y)\n        \n        # Log the experiment results \n        mlflow.log_params(lr_best_params)\n        mlflow.log_metrics(lr_metrics_results)\n        mlflow.sklearn.log_model(lr, artifact_path\u003d\"model\")\n        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Support Vector Machine:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n    mlflow.set_experiment(experiment_name)\n\n    with mlflow.start_run(run_name\u003d\"SVM\"):\n        \n        # Find best hyper parameters using a grid search\n        svm \u003d SVC(random_state \u003d 42)\n        cv \u003d 5\n        params \u003d {\u0027C\u0027: [0.1,1, 10], \u0027gamma\u0027: [1,0.1,0.01,0.001],\u0027kernel\u0027: [\u0027rbf\u0027, \u0027poly\u0027, \u0027sigmoid\u0027]}\n        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n        print(\"Searching for best parameters...\")\n        svm_best_params \u003d model_training.find_best_parameters(X, y, svm, params, cv\u003dcv)\n        print(f\"Best parameters: {svm_best_params}\")\n        \n        # Set the best hyper parameters\n        svm.set_params(**svm_best_params)\n        \n        # Cross evaluate the model on the best hyper parameters\n        svm_metrics_results \u003d model_training.cross_validate_scores(X, y, svm, cv\u003dcv, scoring\u003dscoring)\n        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in svm_metrics_results.items())}\u0027)\n        \n        # Train the model on the whole train set\n        svm.fit(X,y)\n        \n        # Log the experiment results \n        mlflow.log_params(svm_best_params)\n        mlflow.log_metrics(svm_metrics_results)\n        mlflow.sklearn.log_model(svm, artifact_path\u003d\"model\")\n        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Decision Tree:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n    mlflow.set_experiment(experiment_name)\n\n    with mlflow.start_run(run_name\u003d\"Decision Tree\"):\n        \n        # Find best hyper parameters using a grid search\n        dtc \u003d DecisionTreeClassifier(random_state \u003d 42)\n        cv \u003d 5\n        params \u003d {\u0027max_depth\u0027 : [4,5,6,7,8],\n                  \u0027criterion\u0027 :[\u0027gini\u0027, \u0027entropy\u0027]}\n        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n        print(\"Searching for best parameters...\")\n        dtc_best_params \u003d model_training.find_best_parameters(X, y, dtc, params, cv\u003dcv)\n        print(f\"Best parameters: {dtc_best_params}\")\n        \n        # Set the best hyper parameters\n        dtc.set_params(**dtc_best_params)\n        \n        # Cross evaluate the model on the best hyper parameters\n        dtc_metrics_results \u003d model_training.cross_validate_scores(X, y, dtc, cv\u003dcv, scoring\u003dscoring)\n        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in dtc_metrics_results.items())}\u0027)\n        \n        # Train the model on the whole train set\n        dtc.fit(X,y)\n        \n        # Log the experiment results \n        mlflow.log_params(dtc_best_params)\n        mlflow.log_metrics(dtc_metrics_results)\n        mlflow.sklearn.log_model(dtc, artifact_path\u003d\"model\")\n        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Random Forest:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the [Scikit-Learn Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "with project.setup_mlflow(managed_folder\u003dexperiments_managed_folder) as mlflow:\n    mlflow.set_experiment(experiment_name)\n\n    with mlflow.start_run(run_name\u003d\"Random Forest\"):\n        \n        # Find best parameters and cross evaluate the model on the best parameters\n        rfc \u003d RandomForestClassifier(random_state \u003d 42)\n        cv \u003d 5\n        params \u003d {\u0027n_estimators\u0027: [100,200,300],\n                  \u0027max_depth\u0027 : [5,6,7],\n                  \u0027criterion\u0027 :[\u0027gini\u0027, \u0027entropy\u0027]}\n        scoring \u003d [\u0027accuracy\u0027, \u0027precision\u0027, \u0027recall\u0027, \u0027roc_auc\u0027, \u0027f1\u0027]\n        print(\"Searching for best parameters...\")\n        rfc_best_params \u003d model_training.find_best_parameters(X, y, rfc, params, cv\u003dcv)\n        print(f\"Best parameters: {rfc_best_params}\")\n        \n        # Set the best hyper parameters\n        rfc.set_params(**rfc_best_params)\n\n        # Cross evaluate the model on the best hyper parameters\n        rfc_metrics_results \u003d model_training.cross_validate_scores(X, y, rfc, cv\u003dcv, scoring\u003dscoring)\n        print(f\u0027Average values for evaluation metrics after cross validation: {\", \".join(f\"{key}: {round(value, 2)}\" for key, value in rfc_metrics_results.items())}\u0027)\n        \n        # Train the model using the best parameters\n        rfc.fit(X,y)\n        \n        # Log the experiment results \n        mlflow.log_params(rfc_best_params)\n        mlflow.log_metrics(rfc_metrics_results)\n        mlflow.sklearn.log_model(rfc, artifact_path\u003d\"model\")\n        print(\"Best parameters, cross validation metrics, and the model have been saved to Experiment Tracking\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Explore the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-success\"\u003e\n\u003cb\u003eSuccess:\u003c/b\u003e We can now look at the results \u0026 compare our different models by going to the Experiment Tracking page (on the top bar, hover over the circle icon, and select \u003cb\u003eExperiment Tracking\u003c/b\u003e.\n\u003c/div\u003e"
      ]
    }
  ]
}