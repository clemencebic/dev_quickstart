{
  "metadata": {
    "analyzedDataset": "heart_measures",
    "createdOn": 1680705628235,
    "creator": "clemence.bic@dataiku.com",
    "customFields": {},
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "admin",
    "tags": [],
    "versionNumber": 3
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Prepare the input dataset for ML modeling"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## test stuff"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The project is based on the [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).\n\nThis first notebook:\n\n- Performs a quick exploratory analysis of the input dataset: it looks at the structure of the dataset and the distribution of the values in the different categorical and continuous columns.\n\n- Uses the functions from the \u003ca href\u003d\"https://doc.dataiku.com/dss/latest/python/reusing-code.html#sharing-python-code-within-a-project\"\u003eproject Python library\u003c/a\u003e to clean \u0026 prepare the input dataset before Machine Learning modeling. We will first clean categorical and continuous columns, then split the dataset into a train set and a test set.\n\nFinally, we will transform this notebook into a Python recipe in the project Flow that will output the new train and test datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n\u003cb\u003eTip:\u003c/b\u003e The \u003ca href\u003d\"https://doc.dataiku.com/dss/latest/python/reusing-code.html#sharing-python-code-within-a-project\"\u003eproject libraries\u003c/a\u003e allow you to build shared code repositories. They can be synchronized with an external Git repository.\n\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Make sure you\u0027re using the `heart-attack-project` code environment** (see prerequisites)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom heart_attack_library import data_processing\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\nwarnings.filterwarnings(\u0027ignore\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Letâ€™s use the Dataiku Python API to import the input dataset. This piece of code allows retrieving data in the same manner, no matter where the dataset is stored (local filesystem, SQL database, Cloud data lakes, etc.)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "dataset_heart_measures \u003d dataiku.Dataset(\"heart_measures\")\ndf \u003d dataset_heart_measures.get_dataframe(limit\u003d100000)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. A quick audit of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Compute the shape of the dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\u0027The shape of the dataset is {df.shape}\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Look at a preview of the first rows of the dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Inspect missing values \u0026 number of distinct values (cardinality) for each column"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pdu.audit(df)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Define categorical \u0026 continuous columns"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "categorical_cols \u003d [\u0027Sex\u0027,\u0027ChestPainType\u0027, \u0027FastingBS\u0027, \u0027RestingECG\u0027, \u0027ExerciseAngina\u0027, \u0027ST_Slope\u0027]\ncontinuous_cols \u003d [\u0027Age\u0027, \u0027RestingBP\u0027, \u0027Cholesterol\u0027, \u0027MaxHR\u0027, \u0027Oldpeak\u0027]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Look at the distibution of continuous features"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "nb_cols\u003d2\nfig \u003d plt.figure(figsize\u003d(8,6))\nfig.suptitle(\u0027Distribution of continuous features\u0027, fontsize\u003d11)\ngs \u003d fig.add_gridspec(math.ceil(len(continuous_cols)/nb_cols),nb_cols)\ngs.update(wspace\u003d0.3, hspace\u003d0.4)\nfor i, col in enumerate(continuous_cols):\n    ax \u003d fig.add_subplot(gs[math.floor(i/nb_cols),i%nb_cols])\n    sns.histplot(df[col], ax\u003dax)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Look at the distribution of categorical columns"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "nb_cols\u003d2\nfig \u003d plt.figure(figsize\u003d(8,6))\nfig.suptitle(\u0027Distribution of categorical features\u0027, fontsize\u003d11)\ngs \u003d fig.add_gridspec(math.ceil(len(categorical_cols)/nb_cols),nb_cols)\ngs.update(wspace\u003d0.3, hspace\u003d0.4)\nfor i, col in enumerate(categorical_cols):\n    ax \u003d fig.add_subplot(gs[math.floor(i/nb_cols),i%nb_cols])\n    plot \u003d sns.countplot(df[col])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Look at the distribution of target variable"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target \u003d \"HeartDisease\"\nfig \u003d plt.figure(figsize\u003d(4,2.5))\nfig.suptitle(\u0027Distribution of heart attack diseases\u0027, fontsize\u003d11, y\u003d1.11)\nplot \u003d sns.countplot(df[target])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-info\"\u003e\n\u003cb\u003eTip:\u003c/b\u003e To ease collaboration, all the insights you create from Jupyter Notebooks can be\nshared with other users by publishing them on dashboards. See \u003ca href\u003d\"https://doc.dataiku.com/dss/latest/dashboards/insights/jupyter-notebook.html\"\u003edocumentation\u003c/a\u003e for more information.\n\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Clean categorical columns"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": []
      },
      "source": [
        "# Transform string values from categorical columns into int, using the functions from the project libraries\ndf_cleaned \u003d data_processing.transform_heart_categorical_measures(df, \"ChestPainType\", \"RestingECG\", \n                                                                  \"ExerciseAngina\", \"ST_Slope\", \"Sex\")\n\ndf_cleaned.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Transform categorical columns into dummies"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "df_cleaned \u003d pd.get_dummies(df_cleaned, columns \u003d categorical_cols, drop_first \u003d True)\n\nprint(\"Shape after dummies transformation: \" + str(df_cleaned.shape))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Scale continuous columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s use the Scikit-Learn Robust Scaler to scale continuous features"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scaler \u003d RobustScaler()\ndf_cleaned[continuous_cols] \u003d scaler.fit_transform(df_cleaned[continuous_cols])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Split the dataset into train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s now split the dataset into a train set that will be used for experimenting and training the Machine Learning models and test set that will be used to evaluate the deployed model."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "heart_measures_train_df, heart_measures_test_df \u003d train_test_split(df_cleaned, test_size\u003d0.2, stratify\u003ddf_cleaned.HeartDisease)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Next: use this notebook to create a new step in the project workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that our notebook is up and running, we can use it to create the first step of our pipeline in the Flow:\n\n- Click on the **+ Create Recipe** button at the top right of the screen.\n\n- Select the **Python recipe** option.\n\n- Choose the ```heart_measures``` dataset as the input dataset and create two output datasets: ```heart_measures_train``` and ```heart_measures_test```.\n\n- Click on the **Create recipe** button.\n\n- At the end of the recipe script, replace the last four rows of code with:\n\n\n```python\nheart_measures_train \u003d dataiku.Dataset(\"heart_measures_train\")\nheart_measures_train.write_with_schema(heart_measures_train_df)\nheart_measured_test \u003d dataiku.Dataset(\"heart_measures_test\")\nheart_measured_test.write_with_schema(heart_measures_test_df)\n```\n\n- Run the recipe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u003cdiv class\u003d\"alert alert-block alert-success\"\u003e\n\u003cb\u003eSuccess:\u003c/b\u003e We can now go on the Flow, we\u0027ll see an orange circle that represents your first step (we call it a \u0027Recipe\u0027), and two output datasets.\n\u003c/div\u003e"
      ]
    }
  ]
}